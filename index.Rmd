<br> 
<center><img src="https://i.imgur.com/AbCCpQO.png" width="700"></center>


_____

<a id='TOC'></a>

# Tabular data: Univariate Numerical

_____

1. [Counting](#part1)

2. [Measuring](#part2)

3. [LAB (next meeting).](#part3) 


_____

```{r klippy, echo=FALSE, include=TRUE}
klippy::klippy(position = c('top', 'right'))
```

Let's load the data we used for the last session:

```{r getData, eval=TRUE}
rm(list = ls()) # clean memory

location='https://github.com/DACSS-Visual/tabular_univar_cat/raw/main/data/'
file='eduwa.rda'
link=paste0(location,file)


#getting the data TABLE from the file in the cloud:
load(file=url(link))
```

<a id='part1'></a>

## Data from Counting

Counting expresses _integer_ values. They could be represented with bar plots if their frequency table had few discrete values, but that is not generally the case. For example, the variable _Reduced.Lunch_ informs how many kids there are in each school that have that lunch for a reduced price. We have more than 2000 schools, so it is unlikely we find few different values. This is how many different values we have:

```{r unique, eval=TRUE}
# how many unique values
length(unique(eduwa$Reduced.Lunch))
```

These are too many different values. Then, the bar plot is not a good idea (and neither a frequency table), as the bar plot produces a bar for _each_ unique value in the data. This is how a bar plot will look for this variable:

```{r, eval=TRUE}
# from table to dataframe
lunchDF=as.data.frame(table(eduwa$Reduced.Lunch))
names(lunchDF)=c("Reduced_Lunch_Beneficiaries","Count")

# plotting from data frame
library(ggplot2)
base1=ggplot(data=lunchDF, aes(x=Reduced_Lunch_Beneficiaries,Count))
bar1_bad=base1+geom_bar(stat = "identity")
bar1_bad
```

The variety of numerical data requires statistical exploration with some questions in mind:

* What is the representative value? Generally the **mean**, or **median**. 

* How good is the representative value? You need some measure of dispersion. For the mean you have the **standard deviation** and **coefficient of variation**, and for the median the **median absolute deviation**. 

* Does the shape of the distribution differs from a _normal_ distribution? If there is **asymmetry**, is it large enough to show **outliers**? You need some  measure of symmetry and shape (a.k.a. _kurtosis_).


First, let's take a look at the basic statistical information:

```{r summary, eval=TRUE}
summary(eduwa$Reduced.Lunch)
```
This information may give us a general idea on how the data on _reduced lunch_ is behaving, so we select the plot that highlights that behavior. Let me try a box plot:

```{r boxPlotbasic}
base= ggplot(eduwa,aes(y = Reduced.Lunch))  #var to plot in 'y' 
base + geom_boxplot()
```

That is our starting point. Now, let's follow some steps to change the previous boxplot:

1. What should the y-axis show? 

```{r, eval=TRUE}
# get all the summary values but the count of NAs.
(statVals=summary(eduwa$Reduced.Lunch,digits = 3)[1:6])
```
Instead of the default values, we could put those values in the boxplot y-axis:

```{r, eval=TRUE, warning=FALSE, message=FALSE}
library(magrittr)
# the summary values as vector
statVals=statVals%>%as.vector() #notice '%>%'

#plotting
base= ggplot(eduwa,aes(y = Reduced.Lunch))  
b1= base + geom_boxplot() 
b1=b1+ scale_y_continuous(breaks = statVals) #custom breaks
b1
```
In general, flipping can help:

```{r}
b1=b1 +coord_flip()
b1

```

3. Should we annotate?

What about:
* The upper limit (before outliers appear).
* The amount of outliers limit (before outliers appear).

We get the upper limit like this:

```{r, warning=FALSE}
(upperT=ggplot_build(b1)$data[[1]]$ymax)
```

We can compute the _amount of outliers_ on the right tail like this:

```{r, eval=TRUE}
(numOutliers=sum(eduwa$Reduced.Lunch>upperT,na.rm = T))
```

Let's annotate

```{r, eval=TRUE,warning=FALSE, message=FALSE}

# text for annotations
txtOutliers=paste0('#Outlying schools: ',numOutliers)
txtUpper=paste0('Threshold:',upperT)


b1_vertical = b1 + geom_hline(yintercept = upperT,
                            color='red',
                            linetype="dotted",
                            size=2) 
b1_annot=b1_vertical + annotate(geom = 'text',
                                label=txtUpper,
                                y = upperT+5,
                                x=0.2,
                                angle=90) # text angle

b1_annot=b1_annot + annotate(geom = 'text',
                                label=txtOutliers,
                                y = upperT+60,
                                x=0.1,
                                angle=0)
b1_annot
```

3. Do we need the values in the vertical axis?

```{r, eval=TRUE,warning=FALSE, message=FALSE}
b1_annot_noX = b1_annot + 
                theme(axis.text.y = element_blank(),
                      axis.ticks.y = element_blank(),
                      axis.title.y = element_blank())
b1_annot_noX
```

4. What may need further changes?

* What about choosing a different theme (grid)?

```{r}
b1_newGrid=b1_annot_noX +  theme_classic()
b1_newGrid
```

* Maybe the text in the horizontal is not easy to read yet?

```{r}
b1_better_axisText = b1_newGrid+ theme(axis.text.x = element_text(angle = 60,
                                                                  size = 7,
                                                                  vjust = 0.5))
b1_better_axisText
```


In general, numerical data are prone to have outliers in real life (it is not common to speak about outliers in ordinal data since they have few levels).



The **summary** command can be supplemented with some measures of spread or dispersion:

```{r, eval=TRUE}
# standard deviation:
sd(eduwa$Reduced.Lunch,na.rm = T)
```

```{r, eval=TRUE}
# median absolute deviation:
mad(eduwa$Reduced.Lunch,na.rm = T)
```

```{r, warning=FALSE}
# coefficient of variation
library(DescTools)
CoefVar(eduwa$Reduced.Lunch,
        na.rm = T)
```

There are also measures of shape:

```{r, eval=TRUE, message=FALSE}
# asymmetry
Skew(eduwa$Reduced.Lunch,
     na.rm = T)

```

```{r, eval=TRUE}
# kurtosis
Kurt(eduwa$Reduced.Lunch,
     na.rm = T)
```

And the confidence interval for the mean:

```{r, eval=TRUE, message=FALSE}
# confidence interval for the mean
MeanCI(eduwa$Reduced.Lunch,
     na.rm = T)
```

*What do we know right now?*

1. The data is skewed, with a right tail.
2. The data is concentrated in a small group of values.
3. In this situation, the median is more informative than the mean.


You can save those values for future annotations:

```{r}
cv=CoefVar(eduwa$Reduced.Lunch,na.rm = T)
sd=SD(eduwa$Reduced.Lunch,na.rm = T)
md=Median(eduwa$Reduced.Lunch,na.rm = T)
mn=Mean(eduwa$Reduced.Lunch,na.rm = T)
mn.low=MeanCI(eduwa$Reduced.Lunch,
     na.rm = T)[['lwr.ci']]
mn.up=MeanCI(eduwa$Reduced.Lunch,
     na.rm = T)[['upr.ci']]
sk=Skew(eduwa$Reduced.Lunch,
     na.rm = T)
```


A key visual for numerical data is the **histogram**. The histogram looks like a bar plot. In both cases the height of the bars represent counts/percents, but the bars in the histogram are consecutive while the bases of the bars are numeric intervals (**binwidth** informs the width of the intervals). Histograms mainly help you see clearly where and how many _peaks_ there are in the data distribution. Let's make one:

```{r GGLikeBase,eval=TRUE,warning=FALSE, message=FALSE}
#ggplot
barWIDTH=10
library(ggplot2)
base= ggplot(eduwa)  
h1= base + geom_histogram(aes(x = Reduced.Lunch),
                          binwidth = barWIDTH,
                          fill='black') 
h1=h1 + labs(y="count")
h1
```

Let's do some annotations:

```{r, eval=TRUE,warning=FALSE, message=FALSE}
# texts
txtMean=paste0('Mean:',round(mn))
txtSkew=paste0('Skeness:',round(sk,2))

# adding
h1+ geom_vline(xintercept = mn,color='red') + # mean as line
    # about the mean
    annotate(geom = 'text',color='red',
             label=txtMean, # mean as text
              y = 400,
              x=mn+5,
              angle=90) + 
    # about the skewness
    annotate(geom = 'text', color='blue',
             label=txtSkew, # skewness as text
              y = 50,
              x=upperT+170,
              angle=0) 
    
```

* Would you plot the median instead of the mean? 
* How would you use the skewness value?
* What would you do with these facts? 

```{r}
sum(eduwa$Reduced.Lunch==0,na.rm = T)
```
```{r}
sum(eduwa$Reduced.Lunch==0,na.rm = T)/sum(complete.cases(eduwa$Reduced.Lunch))
```



[Go to table of contents.](#TOC)

<a id='part2'></a>

### Measurement

A simplistic idea of measurement tells you the times a particular unit is present in the unit of analysis; which allows for the presence of decimal places. There are variables that could even have negative values.

Let's analyze the variable _Student.Teacher.Ratio_:

```{r summaryMeans, eval=TRUE}
summary(eduwa$Student.Teacher.Ratio)
```

Notice that the maximum value is very far from the mean and the median, this announces the presence of outliers, which can be revealed with a boxplot:

```{r, eval=TRUE,warning=FALSE, message=FALSE}
base=ggplot(eduwa) + theme_light()
box2=base + geom_boxplot(aes(y=Student.Teacher.Ratio)) + 
                      theme(axis.text.y = element_blank(),
                            axis.ticks.y = element_blank(),
                            axis.title.y = element_blank())
box2=box2 + coord_flip()
box2
```

Now, let's see if this is a multimodal distribution (more than one peak). Let's take a look at the histogram:


```{r, warning=FALSE, message=FALSE}
hist2=base + geom_histogram(aes(x=Student.Teacher.Ratio))
hist2=hist2 + labs(y='count')
hist2
```

You can use **ggplot_built** to get more details:

```{r, message=FALSE, warning=FALSE}
ggplot_build(hist2)$data[[1]]%>%head()
```

We can use some of that information:

```{r, message=FALSE, warning=FALSE}
(fromHist=ggplot_build(hist2)$data[[1]][,c('count','x','xmin','xmax')])
```

Here, I want to know the modal class:

```{r}
(modeClassInfo=round(fromHist[which.max(fromHist$count),],2))
```

Let me annotate:

```{r, message=FALSE, warning=FALSE}
# count for modal class
ModeCountY=modeClassInfo$count
# position of modal class 
PositionCountX=modeClassInfo$x
# txt
txtMode=paste0("<- Count of Modal Class [",
                   modeClassInfo$xmin,' - ',
                   modeClassInfo$xmax,']')

hist2ann=hist2 + geom_hline(yintercept =ModeCountY,
                   linetype="dotted") +
    annotate(geom = 'text',
             label=ModeCountY,
              y = ModeCountY+40,
              x=PositionCountX,
             color='red',
              angle=0) +
    annotate(geom = 'text',
             label=txtMode,
              y = ModeCountY+40,
              x=PositionCountX+90,
             color='red',
              angle=0)
hist2ann

```


We know this variable has one peak, it is highly skewed, and it has outliers at the top and the bottom values. We could try to combine all that in one visual:

```{r,warning=FALSE, message=FALSE}
library(ggpubr)
ggarrange(hist2ann,box2,align='v',ncol = 1,heights = 2:1)

```

As you see, that is not an exact match. An alternative can be the **violin** plot.


```{r, warning=FALSE}
base=ggplot(eduwa, aes(x=0,y=Student.Teacher.Ratio))+
            theme_classic()

vio=base+geom_violin(trim=FALSE, fill="orange")

viobox=vio+geom_boxplot(width=0.2)
viobox=viobox + coord_flip()
viobox 
```

We could inform the amount of outliers:

```{r}
theVar=eduwa$Student.Teacher.Ratio
theIQR=IQR(theVar,na.rm = T)
upperT=summary(theVar)[[5]] + theIQR*1.5
lowerT=summary(theVar)[[2]] - theIQR*1.5

# top
(numOutUp=sum(eduwa$Student.Teacher.Ratio>upperT,na.rm = T))
# bottom
(numOutLw=sum(eduwa$Student.Teacher.Ratio<lowerT,na.rm = T))
```

We know there are 185 outliers (83 at the bottom values and 102 at the top). Let's annotate the plot with that information:


```{r, warning=FALSE}
# prepare texts:
annOutUN=paste0(numOutUp,' schools\nabove ',upperT)
annOutLN=paste0(numOutLw,' schools\nbelow ',lowerT)

# plotting
viobox=viobox  + labs(x='')
viobox + annotate(geom = 'text',
              label=annOutUN,
              y = upperT+60,
              x=0.1,
              angle=0) + 
         annotate(geom = 'text',
              label=annOutLN,
              y = lowerT,
              x=0.5,
              angle=0) + 
        theme(axis.text.y = element_blank(), # no text on y
              axis.ticks.y = element_blank(),# no ticks on y
              axis.line.y = element_blank()) # no line on y
```

The violin plot will inform the presence of peaks, but not on the maximum value of the peak as the histogram shows.

_____

[Go to table of contents.](#TOC)

_____

<a id='part3'></a>


## LAB for THURSDAY: From numeric to ordinal via aggregation

Numerical data may be difficult to communicate as it has a lot of variability, and range. We could cut the data into intervals as in the last week´s lab; but you will get a histogram. 

Alternatively, you can use another variable, in general a political/geographical unit, to summarize the data:

```{r}
# sum of reduced lunches given per county
reducedLunch_county=aggregate(data=eduwa,
                         Reduced.Lunch~County,
                         FUN=sum)
#see
reducedLunch_county
```

Now you have much less cases, just 39. Let's order by count in decreasing order:

```{r}
# order and minus (-) for decreasing
reducedLunch_county=reducedLunch_county[order(-reducedLunch_county$Reduced.Lunch),]
row.names(reducedLunch_county)=NULL
head(reducedLunch_county,10)
```
Let's add the percent, and cummulative percent and cummulative count:

```{r}
reducedLunch_county$Percent=reducedLunch_county$Reduced.Lunch/sum(reducedLunch_county$Reduced.Lunch)
reducedLunch_county$PercentCum=cumsum(reducedLunch_county$Percent)
reducedLunch_county$Reduced.Lunch.Cum=cumsum(reducedLunch_county$Reduced.Lunch)
# you get:
reducedLunch_county
```

Now, instead the statistics of the numerical values (count of beneficiaries), you have information on the distribution of beneficiaries accross counties. The table below can tell you:

```{r}
# most benefited counties (sum the 80% of reduced lunches)
reducedLunch_county[reducedLunch_county$PercentCum<0.8,"County"]
```
```{r}
# count of the top benefited
length(reducedLunch_county[reducedLunch_county$PercentCum<0.8,"County"])
```

```{r}
# share represented of the top benefited
length(reducedLunch_county[reducedLunch_county$PercentCum<0.8,"County"])/nrow(reducedLunch_county)
```

You can see that in a Pareto plot:

```{r}
library(ggQC) # install this previously

base=ggplot(data=reducedLunch_county,
             aes(x=County,y=Reduced.Lunch)) + # just the counts
     theme_classic()
paretoDraft=base + stat_pareto() 
paretoDraft

```

Please, prepare for Thursday one of these:

- A **better** version of this Pareto plot (modify it).

- Your  **own alternative** version of a Pareto plot for the _Reduced.Lunch_ variable without using the library **ggQC** . This would be your first step:


```{r, eval=TRUE}

myBase=ggplot(data=reducedLunch_county,
             aes(x=County,Reduced.Lunch.Cum)) + theme_classic() 
myBarDraft=myBase  +geom_bar(stat = "identity") 
myBarDraft=myBarDraft + coord_flip() 
myBarDraft
```



_____

[Go to table of contents.](#part0)
